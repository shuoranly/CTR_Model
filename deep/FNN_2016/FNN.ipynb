{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict, namedtuple\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_data = pd.read_csv('../../data/criteo_data/criteo_sample.txt', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>...</th>\n",
       "      <th>C17</th>\n",
       "      <th>C18</th>\n",
       "      <th>C19</th>\n",
       "      <th>C20</th>\n",
       "      <th>C21</th>\n",
       "      <th>C22</th>\n",
       "      <th>C23</th>\n",
       "      <th>C24</th>\n",
       "      <th>C25</th>\n",
       "      <th>C26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>260.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17668.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>87c6f83c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0429f84b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3a171ecb</td>\n",
       "      <td>c0d61a5c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>30251.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>...</td>\n",
       "      <td>d4bb7bd8</td>\n",
       "      <td>6fc84bfb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5155d8a3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>be7c41b4</td>\n",
       "      <td>ded4aac9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>...</td>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>675c9258</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2e01979f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bcdee96c</td>\n",
       "      <td>6d5d1302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16836.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>52e44668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e587c466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>3b183c5c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>...</td>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>25c88e42</td>\n",
       "      <td>21ddcdc9</td>\n",
       "      <td>b1252a9d</td>\n",
       "      <td>0e8585d2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>0d4a6d1a</td>\n",
       "      <td>001f3601</td>\n",
       "      <td>92c878de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label   I1  I2     I3    I4       I5     I6   I7    I8     I9  ...  \\\n",
       "0      0  NaN   3  260.0   NaN  17668.0    NaN  NaN  33.0    NaN  ...   \n",
       "1      0  NaN  -1   19.0  35.0  30251.0  247.0  1.0  35.0  160.0  ...   \n",
       "2      0  0.0   0    2.0  12.0   2013.0  164.0  6.0  35.0  523.0  ...   \n",
       "3      0  NaN  13    1.0   4.0  16836.0  200.0  5.0   4.0   29.0  ...   \n",
       "4      0  0.0   0  104.0  27.0   1990.0  142.0  4.0  32.0   37.0  ...   \n",
       "\n",
       "        C17       C18       C19       C20       C21  C22       C23       C24  \\\n",
       "0  e5ba7672  87c6f83c       NaN       NaN  0429f84b  NaN  3a171ecb  c0d61a5c   \n",
       "1  d4bb7bd8  6fc84bfb       NaN       NaN  5155d8a3  NaN  be7c41b4  ded4aac9   \n",
       "2  e5ba7672  675c9258       NaN       NaN  2e01979f  NaN  bcdee96c  6d5d1302   \n",
       "3  e5ba7672  52e44668       NaN       NaN  e587c466  NaN  32c7478e  3b183c5c   \n",
       "4  e5ba7672  25c88e42  21ddcdc9  b1252a9d  0e8585d2  NaN  32c7478e  0d4a6d1a   \n",
       "\n",
       "        C25       C26  \n",
       "0       NaN       NaN  \n",
       "1       NaN       NaN  \n",
       "2       NaN       NaN  \n",
       "3       NaN       NaN  \n",
       "4  001f3601  92c878de  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = ['C' + str(i) for i in range(1, 27)]\n",
    "dense_features = ['I' + str(i) for i in range(1, 14)]\n",
    "sam_data[sparse_features] = sam_data[sparse_features].fillna('-1', )\n",
    "sam_data[dense_features] = sam_data[dense_features].fillna(0, )\n",
    "target = ['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    sam_data[feat] = lbe.fit_transform(sam_data[feat])\n",
    "mms = MinMaxScaler(feature_range=(0, 1))\n",
    "sam_data[dense_features] = mms.fit_transform(sam_data[dense_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseFeat(namedtuple('SparseFeat', ['name', 'dimension', 'dtype','embedding_name','embedding'])):\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __new__(cls, name, dimension, dtype=\"int32\", embedding_name=None, embedding=True):\n",
    "        if embedding and embedding_name is None:\n",
    "            embedding_name = name\n",
    "        return super(SparseFeat, cls).__new__(cls, name, dimension, dtype, embedding_name,embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseFeat(namedtuple('DenseFeat', ['name', 'dimension', 'dtype'])):\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __new__(cls, name, dimension=1, dtype=\"float32\"):\n",
    "\n",
    "        return super(DenseFeat, cls).__new__(cls, name, dimension, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarLenSparseFeat(namedtuple('VarLenFeat', ['name', 'dimension', 'maxlen', 'combiner', 'dtype','embedding_name','embedding'])):\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __new__(cls, name, dimension, maxlen, combiner=\"mean\", dtype=\"float32\", embedding_name=None,embedding=True):\n",
    "        if embedding_name is None:\n",
    "            embedding_name = name\n",
    "        return super(VarLenSparseFeat, cls).__new__(cls, name, dimension, maxlen, combiner, dtype, embedding_name, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_features(feature_columns, include_varlen=True, mask_zero=True, prefix='',include_fixlen=True):\n",
    "    input_features = OrderedDict()\n",
    "    if include_fixlen:\n",
    "        for fc in feature_columns:\n",
    "            if isinstance(fc,SparseFeat):\n",
    "                input_features[fc.name] = tf.keras.Input(\n",
    "                    shape=(1,), name=prefix+fc.name, dtype=fc.dtype)\n",
    "            elif isinstance(fc,DenseFeat):\n",
    "                input_features[fc.name] = tf.keras.Input(\n",
    "                    shape=(fc.dimension,), name=prefix + fc.name, dtype=fc.dtype)\n",
    "    if include_varlen:\n",
    "        for fc in feature_columns:\n",
    "            if isinstance(fc,VarLenSparseFeat):\n",
    "                input_features[fc.name] = tf.keras.Input(shape=(fc.maxlen,), name=prefix + 'seq_' + fc.name,\n",
    "                                                      dtype=fc.dtype)\n",
    "        if not mask_zero:\n",
    "            for fc in feature_columns:\n",
    "                input_features[fc.name+\"_seq_length\"] = tf.keras.Input(shape=(\n",
    "                    1,), name=prefix + 'seq_length_' + fc.name)\n",
    "                input_features[fc.name+\"_seq_max_length\"] = fc.maxlen\n",
    "\n",
    "\n",
    "    return input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fixlen_feature_names(feature_columns):\n",
    "    features = build_input_features(feature_columns, include_varlen=False,include_fixlen=True)\n",
    "    return list(features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_varlen_feature_names(feature_columns):\n",
    "    features = build_input_features(feature_columns, include_varlen=True,include_fixlen=False)\n",
    "    return list(features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs_list(inputs):\n",
    "    return list(chain(*list(map(lambda x: x.values(), filter(lambda x: x is not None, inputs)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_dict(sparse_feature_columns, varlen_sparse_feature_columns, embedding_size, init_std, seed, l2_reg,\n",
    "                          prefix='sparse_', seq_mask_zero=True):\n",
    "    if embedding_size == 'auto':\n",
    "        print(\"Notice:Do not use auto embedding in models other than DCN\")\n",
    "        sparse_embedding = {feat.embedding_name: tf.keras.layers.Embedding(feat.dimension, 6 * int(pow(feat.dimension, 0.25)),\n",
    "                                                 embeddings_initializer=tf.keras.initializers.RandomNormal(\n",
    "                                                     mean=0.0, stddev=init_std, seed=seed),\n",
    "                                                 embeddings_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                                                 name=prefix + '_emb_' + feat.name) for feat in sparse_feature_columns}\n",
    "    else:\n",
    "        sparse_embedding = {feat.embedding_name: tf.keras.layers.Embedding(feat.dimension, embedding_size,\n",
    "                                                 embeddings_initializer=tf.keras.initializers.RandomNormal(\n",
    "                                                     mean=0.0, stddev=init_std, seed=seed),\n",
    "                                                 embeddings_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                                                 name=prefix + '_emb_'  + feat.name) for feat in sparse_feature_columns}\n",
    "\n",
    "    if varlen_sparse_feature_columns and len(varlen_sparse_feature_columns) > 0:\n",
    "        for feat in varlen_sparse_feature_columns:\n",
    "            # if feat.name not in sparse_embedding:\n",
    "            if embedding_size == \"auto\":\n",
    "                sparse_embedding[feat.embedding_name] = tf.keras.layers.Embedding(feat.dimension, 6 * int(pow(feat.dimension, 0.25)),\n",
    "                                                        embeddings_initializer=tf.keras.initializers.RandomNormal(\n",
    "                                                            mean=0.0, stddev=init_std, seed=seed),\n",
    "                                                        embeddings_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                                                        name=prefix + '_seq_emb_' + feat.name,\n",
    "                                                        mask_zero=seq_mask_zero)\n",
    "\n",
    "            else:\n",
    "                sparse_embedding[feat.embedding_name] = tf.keras.layers.Embedding(feat.dimension, embedding_size,\n",
    "                                                        embeddings_initializer=tf.keras.initializers.RandomNormal(\n",
    "                                                            mean=0.0, stddev=init_std, seed=seed),\n",
    "                                                        embeddings_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                                                        name=prefix + '_seq_emb_' + feat.name,\n",
    "                                                        mask_zero=seq_mask_zero)\n",
    "\n",
    "\n",
    "    return sparse_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_vec_list(embedding_dict, input_dict, sparse_feature_columns, return_feat_list=(), mask_feat_list=()):\n",
    "    embedding_vec_list = []\n",
    "    for fg in sparse_feature_columns:\n",
    "        feat_name = fg.name\n",
    "        if len(return_feat_list) == 0  or feat_name in return_feat_list:\n",
    "            lookup_idx = input_dict[feat_name]\n",
    "            embedding_vec_list.append(embedding_dict[feat_name](lookup_idx))\n",
    "\n",
    "    return embedding_vec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(feature_columns, l2_reg, init_std, seed, embedding_size, prefix=\"\",seq_mask_zero=True):\n",
    "    sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, SparseFeat) and x.embedding, feature_columns)) if feature_columns else []\n",
    "    varlen_sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, VarLenSparseFeat) and x.embedding, feature_columns)) if feature_columns else []\n",
    "    sparse_emb_dict = create_embedding_dict(sparse_feature_columns, varlen_sparse_feature_columns, embedding_size, init_std, seed,\n",
    "                                                 l2_reg, prefix=prefix + 'sparse',seq_mask_zero=seq_mask_zero)\n",
    "    return sparse_emb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_lookup(sparse_embedding_dict, sparse_input_dict, sparse_feature_columns, return_feat_list=(), mask_feat_list=()):\n",
    "    embedding_vec_list = []\n",
    "    for fc in sparse_feature_columns:\n",
    "        feature_name = fc.name\n",
    "        embedding_name = fc.embedding_name\n",
    "        if len(return_feat_list) == 0  or feature_name in return_feat_list and fc.embedding:\n",
    "            lookup_idx = sparse_input_dict[feature_name]\n",
    "            embedding_vec_list.append(sparse_embedding_dict[embedding_name](lookup_idx))\n",
    "\n",
    "    return embedding_vec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def varlen_embedding_lookup(embedding_dict, sequence_input_dict, varlen_sparse_feature_columns):\n",
    "    varlen_embedding_vec_dict = {}\n",
    "    for fc in varlen_sparse_feature_columns:\n",
    "        feature_name = fc.name\n",
    "        embedding_name = fc.embedding_name\n",
    "        lookup_idx = sequence_input_dict[feature_name]\n",
    "        varlen_embedding_vec_dict[feature_name] = embedding_dict[embedding_name](lookup_idx)\n",
    "\n",
    "    return varlen_embedding_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequencePoolingLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"The SequencePoolingLayer is used to apply pooling operation(sum,mean,max) on variable-length sequence feature/multi-value feature.\n",
    "\n",
    "      Input shape\n",
    "        - A list of two  tensor [seq_value,seq_len]\n",
    "\n",
    "        - seq_value is a 3D tensor with shape: ``(batch_size, T, embedding_size)``\n",
    "\n",
    "        - seq_len is a 2D tensor with shape : ``(batch_size, 1)``,indicate valid length of each sequence.\n",
    "\n",
    "      Output shape\n",
    "        - 3D tensor with shape: ``(batch_size, 1, embedding_size)``.\n",
    "\n",
    "      Arguments\n",
    "        - **mode**:str.Pooling operation to be used,can be sum,mean or max.\n",
    "\n",
    "        - **supports_masking**:If True,the input need to support masking.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mode='mean', supports_masking=False, **kwargs):\n",
    "\n",
    "        if mode not in ['sum', 'mean', 'max']:\n",
    "            raise ValueError(\"mode must be sum or mean\")\n",
    "        self.mode = mode\n",
    "        self.eps = 1e-8\n",
    "        super(SequencePoolingLayer, self).__init__(**kwargs)\n",
    "\n",
    "        self.supports_masking = supports_masking\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if not self.supports_masking:\n",
    "            self.seq_len_max = int(input_shape[0][1])\n",
    "        super(SequencePoolingLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, seq_value_len_list, mask=None, **kwargs):\n",
    "        if self.supports_masking:\n",
    "            if mask is None:\n",
    "                raise ValueError(\n",
    "                    \"When supports_masking=True,input must support masking\")\n",
    "            uiseq_embed_list = seq_value_len_list\n",
    "            mask = tf.cast(mask,tf.float32)#                tf.to_float(mask)\n",
    "            user_behavior_length = reduce_sum(mask, axis=-1, keep_dims=True)\n",
    "            mask = tf.expand_dims(mask, axis=2)\n",
    "        else:\n",
    "            uiseq_embed_list, user_behavior_length = seq_value_len_list\n",
    "\n",
    "            mask = tf.sequence_mask(user_behavior_length,\n",
    "                                    self.seq_len_max, dtype=tf.float32)\n",
    "            mask = tf.transpose(mask, (0, 2, 1))\n",
    "\n",
    "        embedding_size = uiseq_embed_list.shape[-1]\n",
    "\n",
    "        mask = tf.tile(mask, [1, 1, embedding_size])\n",
    "\n",
    "        uiseq_embed_list *= mask\n",
    "        hist = uiseq_embed_list\n",
    "        if self.mode == \"max\":\n",
    "            return reduce_max(hist, 1, keep_dims=True)\n",
    "\n",
    "        hist = reduce_sum(hist, 1, keep_dims=False)\n",
    "\n",
    "        if self.mode == \"mean\":\n",
    "            hist = div(hist, user_behavior_length + self.eps)\n",
    "\n",
    "        hist = tf.expand_dims(hist, axis=1)\n",
    "        return hist\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.supports_masking:\n",
    "            return (None, 1, input_shape[-1])\n",
    "        else:\n",
    "            return (None, 1, input_shape[0][-1])\n",
    "\n",
    "    def compute_mask(self, inputs, mask):\n",
    "        return None\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'mode': self.mode, 'supports_masking': self.supports_masking}\n",
    "        base_config = super(SequencePoolingLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_varlen_pooling_list(embedding_dict, features, varlen_sparse_feature_columns):\n",
    "    pooling_vec_list = []\n",
    "    for fc in varlen_sparse_feature_columns:\n",
    "        feature_name = fc.name\n",
    "        combiner = fc.combiner\n",
    "        feature_length_name = feature_name + '_seq_length'\n",
    "        if feature_length_name in features:\n",
    "            vec = SequencePoolingLayer(combiner, supports_masking=False)(\n",
    "            [embedding_dict[feature_name], features[feature_length_name]])\n",
    "        else:\n",
    "            vec = SequencePoolingLayer(combiner, supports_masking=True)(\n",
    "            embedding_dict[feature_name])\n",
    "        pooling_vec_list.append(vec)\n",
    "    return pooling_vec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense_input(features,feature_columns):\n",
    "    dense_feature_columns = list(filter(lambda x:isinstance(x,DenseFeat),feature_columns)) if feature_columns else []\n",
    "    dense_input_list = []\n",
    "    for fc in dense_feature_columns:\n",
    "        dense_input_list.append(features[fc.name])\n",
    "    return dense_input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_from_feature_columns(features,feature_columns, embedding_size, l2_reg, init_std, seed,prefix='',seq_mask_zero=True,support_dense=True):\n",
    "\n",
    "\n",
    "    sparse_feature_columns = list(filter(lambda x:isinstance(x,SparseFeat),feature_columns)) if feature_columns else []\n",
    "    varlen_sparse_feature_columns = list(filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if feature_columns else []\n",
    "\n",
    "    embedding_dict = create_embedding_matrix(feature_columns,l2_reg,init_std,seed,embedding_size, prefix=prefix,seq_mask_zero=seq_mask_zero)\n",
    "    sparse_embedding_list = embedding_lookup(\n",
    "        embedding_dict, features, sparse_feature_columns)\n",
    "    dense_value_list = get_dense_input(features,feature_columns)\n",
    "    if not support_dense and len(dense_value_list) >0:\n",
    "        raise ValueError(\"DenseFeat is not supported in dnn_feature_columns\")\n",
    "\n",
    "    sequence_embed_dict = varlen_embedding_lookup(embedding_dict,features,varlen_sparse_feature_columns)\n",
    "    sequence_embed_list = get_varlen_pooling_list(sequence_embed_dict, features, varlen_sparse_feature_columns)\n",
    "    sparse_embedding_list += sequence_embed_list\n",
    "\n",
    "    return sparse_embedding_list, dense_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_fun(inputs, axis=-1):\n",
    "    if len(inputs) == 1:\n",
    "        return inputs[0]\n",
    "    else:\n",
    "        return tf.keras.layers.Concatenate(axis=axis)(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_dnn_input(sparse_embedding_list,dense_value_list):\n",
    "    if len(sparse_embedding_list) > 0 and len(dense_value_list) > 0:\n",
    "        sparse_dnn_input = tf.keras.layers.Flatten()(concat_fun(sparse_embedding_list))\n",
    "        dense_dnn_input = tf.keras.layers.Flatten()(concat_fun(dense_value_list))\n",
    "        return concat_fun([sparse_dnn_input,dense_dnn_input])\n",
    "    elif len(sparse_embedding_list) > 0:\n",
    "        return tf.keras.layers.Flatten()(concat_fun(sparse_embedding_list))\n",
    "    elif len(dense_value_list) > 0:\n",
    "        return tf.keras.layers.Flatten()(concat_fun(dense_value_list))\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reduce_mean(input_tensor,\n",
    "               axis=None,\n",
    "               keep_dims=False,\n",
    "               name=None,\n",
    "               reduction_indices=None):\n",
    "    if tf.__version__ < '2.0.0':\n",
    "        return tf.reduce_mean(input_tensor,\n",
    "                   axis=axis,\n",
    "                   keep_dims=keep_dims,\n",
    "                   name=name,\n",
    "                   reduction_indices=reduction_indices)\n",
    "    else:\n",
    "        return  tf.reduce_mean(input_tensor,\n",
    "                   axis=axis,\n",
    "                   keepdims=keep_dims,\n",
    "                   name=name)\n",
    "\n",
    "\n",
    "def reduce_sum(input_tensor,\n",
    "               axis=None,\n",
    "               keep_dims=False,\n",
    "               name=None,\n",
    "               reduction_indices=None):\n",
    "    if tf.__version__ < '2.0.0':\n",
    "        return tf.reduce_sum(input_tensor,\n",
    "                   axis=axis,\n",
    "                   keep_dims=keep_dims,\n",
    "                   name=name,\n",
    "                   reduction_indices=reduction_indices)\n",
    "    else:\n",
    "        return  tf.reduce_sum(input_tensor,\n",
    "                   axis=axis,\n",
    "                   keepdims=keep_dims,\n",
    "                   name=name)\n",
    "\n",
    "def reduce_max(input_tensor,\n",
    "               axis=None,\n",
    "               keep_dims=False,\n",
    "               name=None,\n",
    "               reduction_indices=None):\n",
    "    if tf.__version__ < '2.0.0':\n",
    "        return tf.reduce_max(input_tensor,\n",
    "                   axis=axis,\n",
    "                   keep_dims=keep_dims,\n",
    "                   name=name,\n",
    "                   reduction_indices=reduction_indices)\n",
    "    else:\n",
    "        return  tf.reduce_max(input_tensor,\n",
    "                   axis=axis,\n",
    "                   keepdims=keep_dims,\n",
    "                   name=name)\n",
    "\n",
    "def div(x, y, name=None):\n",
    "    if tf.__version__ < '2.0.0':\n",
    "        return tf.div(x, y, name=name)\n",
    "    else:\n",
    "        return tf.divide(x, y, name=name)\n",
    "\n",
    "def softmax(logits, dim=-1, name=None):\n",
    "    if tf.__version__ < '2.0.0':\n",
    "        return tf.nn.softmax(logits, dim=dim, name=name)\n",
    "    else:\n",
    "        return tf.nn.softmax(logits, axis=dim, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, l2_reg=0.0, mode=0, **kwargs):\n",
    "\n",
    "        self.l2_reg = l2_reg\n",
    "        self.mode = mode\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self.bias = self.add_weight(name='linear_bias',\n",
    "                                    shape=(1,),\n",
    "                                    initializer=tf.keras.initializers.Zeros(),\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=1, activation=None, use_bias=False,\n",
    "                                           kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg))\n",
    "\n",
    "        super(Linear, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, inputs , **kwargs):\n",
    "\n",
    "        if self.mode == 0:\n",
    "            sparse_input = inputs\n",
    "            linear_logit = reduce_sum(sparse_input, axis=-1, keep_dims=True)\n",
    "        elif self.mode == 1:\n",
    "            dense_input = inputs\n",
    "            linear_logit = self.dense(dense_input)\n",
    "\n",
    "        else:\n",
    "            sparse_input, dense_input = inputs\n",
    "\n",
    "            linear_logit = reduce_sum(sparse_input, axis=-1, keep_dims=False) + self.dense(dense_input)\n",
    "\n",
    "        linear_bias_logit = linear_logit + self.bias\n",
    "\n",
    "        return linear_bias_logit\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 1)\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'mode': self.mode, 'l2_reg': self.l2_reg}\n",
    "        base_config = super(Linear, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_logit(features, feature_columns, units=1, l2_reg=0, init_std=0.0001, seed=1024, prefix='linear'):\n",
    "\n",
    "    linear_emb_list = [input_from_feature_columns(features,feature_columns,1,l2_reg,init_std,seed,prefix=prefix+str(i))[0] for i in range(units)]\n",
    "    _, dense_input_list = input_from_feature_columns(features,feature_columns,1,l2_reg,init_std,seed,prefix=prefix)\n",
    "\n",
    "    linear_logit_list = []\n",
    "    for i in range(units):\n",
    "\n",
    "        if len(linear_emb_list[0])>0 and len(dense_input_list) >0:\n",
    "            sparse_input = concat_fun(linear_emb_list[i])\n",
    "            dense_input = concat_fun(dense_input_list)\n",
    "            linear_logit = Linear(l2_reg,mode=2)([sparse_input,dense_input])\n",
    "        elif len(linear_emb_list[0])>0:\n",
    "            sparse_input = concat_fun(linear_emb_list[i])\n",
    "            linear_logit = Linear(l2_reg,mode=0)(sparse_input)\n",
    "        elif len(dense_input_list) >0:\n",
    "            dense_input = concat_fun(dense_input_list)\n",
    "            linear_logit = Linear(l2_reg,mode=1)(dense_input)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        linear_logit_list.append(linear_logit)\n",
    "\n",
    "    return concat_fun(linear_logit_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dice(tf.keras.layers.Layer):\n",
    "    \"\"\"The Data Adaptive Activation Function in DIN,which can be viewed as a generalization of PReLu and can adaptively adjust the rectified point according to distribution of input data.\n",
    "\n",
    "      Input shape\n",
    "        - Arbitrary. Use the keyword argument `input_shape` (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.\n",
    "\n",
    "      Output shape\n",
    "        - Same shape as the input.\n",
    "\n",
    "      Arguments\n",
    "        - **axis** : Integer, the axis that should be used to compute data distribution (typically the features axis).\n",
    "\n",
    "        - **epsilon** : Small float added to variance to avoid dividing by zero.\n",
    "\n",
    "      References\n",
    "        - [Zhou G, Zhu X, Song C, et al. Deep interest network for click-through rate prediction[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2018: 1059-1068.](https://arxiv.org/pdf/1706.06978.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, axis=-1, epsilon=1e-9, **kwargs):\n",
    "        self.axis = axis\n",
    "        self.epsilon = epsilon\n",
    "        super(Dice, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bn = tf.keras.layers.BatchNormalization(\n",
    "            axis=self.axis, epsilon=self.epsilon, center=False, scale=False)\n",
    "        self.alphas = self.add_weight(shape=(input_shape[-1],), initializer=tf.keras.initializers.Zeros(\n",
    "        ), dtype=tf.float32, name= 'dice_alpha')  # name='alpha_'+self.name\n",
    "        super(Dice, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "        self.uses_learning_phase = True\n",
    "\n",
    "    def call(self, inputs,training=None,**kwargs):\n",
    "        inputs_normed = self.bn(inputs,training=training)\n",
    "        # tf.layers.batch_normalization(\n",
    "        # inputs, axis=self.axis, epsilon=self.epsilon, center=False, scale=False)\n",
    "        x_p = tf.sigmoid(inputs_normed)\n",
    "        return self.alphas * (1.0 - x_p) * inputs + x_p * inputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'axis': self.axis, 'epsilon': self.epsilon}\n",
    "        base_config = super(Dice, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "def activation_layer(activation):\n",
    "    if activation == \"dice\" or activation == \"Dice\":\n",
    "        act_layer =  Dice()\n",
    "    elif (isinstance(activation, str)) or (sys.version_info.major == 2 and isinstance(activation, (str, unicode))):\n",
    "        act_layer = tf.keras.layers.Activation(activation)\n",
    "    elif issubclass(activation, Layer):\n",
    "        act_layer = activation()\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Invalid activation,found %s.You should use a str or a Activation Layer Class.\" % (activation))\n",
    "    return act_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(tf.keras.layers.Layer):\n",
    "    \"\"\"The Multi Layer Percetron\n",
    "\n",
    "      Input shape\n",
    "        - nD tensor with shape: ``(batch_size, ..., input_dim)``. The most common situation would be a 2D input with shape ``(batch_size, input_dim)``.\n",
    "\n",
    "      Output shape\n",
    "        - nD tensor with shape: ``(batch_size, ..., hidden_size[-1])``. For instance, for a 2D input with shape ``(batch_size, input_dim)``, the output would have shape ``(batch_size, hidden_size[-1])``.\n",
    "\n",
    "      Arguments\n",
    "        - **hidden_units**:list of positive integer, the layer number and units in each layer.\n",
    "\n",
    "        - **activation**: Activation function to use.\n",
    "\n",
    "        - **l2_reg**: float between 0 and 1. L2 regularizer strength applied to the kernel weights matrix.\n",
    "\n",
    "        - **dropout_rate**: float in [0,1). Fraction of the units to dropout.\n",
    "\n",
    "        - **use_bn**: bool. Whether use BatchNormalization before activation or not.\n",
    "\n",
    "        - **seed**: A Python integer to use as random seed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_units, activation='relu', l2_reg=0, dropout_rate=0, use_bn=False, seed=1024, **kwargs):\n",
    "        self.hidden_units = hidden_units\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.seed = seed\n",
    "        self.l2_reg = l2_reg\n",
    "        self.use_bn = use_bn\n",
    "        super(DNN, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_size = input_shape[-1]\n",
    "        hidden_units = [int(input_size)] + list(self.hidden_units)\n",
    "        self.kernels = [self.add_weight(name='kernel' + str(i),\n",
    "                                        shape=(\n",
    "                                            hidden_units[i], hidden_units[i + 1]),\n",
    "                                        regularizer=tf.keras.regularizers.l2(self.l2_reg),\n",
    "                                        trainable=True) for i in range(len(self.hidden_units))]\n",
    "        self.bias = [self.add_weight(name='bias' + str(i),\n",
    "                                     shape=(self.hidden_units[i],),\n",
    "                                     initializer=tf.keras.initializers.Zeros(),\n",
    "                                     trainable=True) for i in range(len(self.hidden_units))]\n",
    "        if self.use_bn:\n",
    "            self.bn_layers = [tf.keras.layers.BatchNormalization() for _ in range(len(self.hidden_units))]\n",
    "\n",
    "        self.dropout_layers = [tf.keras.layers.Dropout(self.dropout_rate,seed=self.seed+i) for i in range(len(self.hidden_units))]\n",
    "\n",
    "        self.activation_layers = [activation_layer(self.activation) for _ in range(len(self.hidden_units))]\n",
    "\n",
    "        super(DNN, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "\n",
    "        deep_input = inputs\n",
    "\n",
    "        for i in range(len(self.hidden_units)):\n",
    "            fc = tf.nn.bias_add(tf.tensordot(\n",
    "                deep_input, self.kernels[i], axes=(-1, 0)), self.bias[i])\n",
    "            # fc = Dense(self.hidden_size[i], activation=None, \\\n",
    "            #           kernel_initializer=glorot_normal(seed=self.seed), \\\n",
    "            #           kernel_regularizer=l2(self.l2_reg))(deep_input)\n",
    "            if self.use_bn:\n",
    "                fc = self.bn_layers[i](fc, training=training)\n",
    "\n",
    "            fc = self.activation_layers[i](fc)\n",
    "\n",
    "            fc = self.dropout_layers[i](fc,training = training)\n",
    "            deep_input = fc\n",
    "\n",
    "        return deep_input\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if len(self.hidden_units) > 0:\n",
    "            shape = input_shape[:-1] + (self.hidden_units[-1],)\n",
    "        else:\n",
    "            shape = input_shape\n",
    "\n",
    "        return tuple(shape)\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'activation': self.activation, 'hidden_units': self.hidden_units,\n",
    "                  'l2_reg': self.l2_reg, 'use_bn': self.use_bn, 'dropout_rate': self.dropout_rate, 'seed': self.seed}\n",
    "        base_config = super(DNN, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "      Arguments\n",
    "         - **task**: str, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss\n",
    "\n",
    "         - **use_bias**: bool.Whether add bias term or not.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, task='binary', use_bias=True, **kwargs):\n",
    "        if task not in [\"binary\", \"multiclass\", \"regression\"]:\n",
    "            raise ValueError(\"task must be binary,multiclass or regression\")\n",
    "        self.task = task\n",
    "        self.use_bias = use_bias\n",
    "        super(PredictionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.global_bias = self.add_weight(\n",
    "                shape=(1,), initializer=tf.keras.initializers.Zeros(), name=\"global_bias\")\n",
    "\n",
    "        # Be sure to call this somewhere!\n",
    "        super(PredictionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = inputs\n",
    "        if self.use_bias:\n",
    "            x = tf.nn.bias_add(x, self.global_bias, data_format='NHWC')\n",
    "        if self.task == \"binary\":\n",
    "            x = tf.sigmoid(x)\n",
    "\n",
    "        output = tf.reshape(x, (-1, 1))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 1)\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'task': self.task, 'use_bias': self.use_bias}\n",
    "        base_config = super(PredictionLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FNN(linear_feature_columns, dnn_feature_columns, embedding_size=8, dnn_hidden_units=(128, 128),\n",
    "        l2_reg_embedding=1e-5, l2_reg_linear=1e-5, l2_reg_dnn=0, init_std=0.0001, seed=1024, dnn_dropout=0,\n",
    "        dnn_activation='relu', task='binary'):\n",
    "    \"\"\"Instantiates the Factorization-supported Neural Network architecture.\n",
    "\n",
    "    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.\n",
    "    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n",
    "    :param embedding_size: positive integer,sparse feature embedding_size\n",
    "    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of deep net\n",
    "    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n",
    "    :param l2_reg_linear: float. L2 regularizer strength applied to linear weight\n",
    "    :param l2_reg_dnn: float . L2 regularizer strength applied to DNN\n",
    "    :param init_std: float,to use as the initialize std of embedding vector\n",
    "    :param seed: integer ,to use as random seed.\n",
    "    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n",
    "    :param dnn_activation: Activation function to use in DNN\n",
    "    :param task: str, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss\n",
    "    :return: A Keras model instance.\n",
    "    \"\"\"\n",
    "    features = build_input_features(linear_feature_columns + dnn_feature_columns)\n",
    "    \n",
    "    inputs_list = list(features.values())\n",
    "\n",
    "    sparse_embedding_list, dense_value_list = input_from_feature_columns(features,dnn_feature_columns,\n",
    "                                                                              embedding_size,\n",
    "                                                                              l2_reg_embedding,init_std,\n",
    "                                                                              seed)\n",
    "\n",
    "\n",
    "    linear_logit = get_linear_logit(features, linear_feature_columns, l2_reg=l2_reg_linear, init_std=init_std,\n",
    "                                    seed=seed, prefix='linear')\n",
    "\n",
    "\n",
    "    dnn_input = combined_dnn_input(sparse_embedding_list,dense_value_list)\n",
    "   \n",
    "    deep_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn,\n",
    "                   dnn_dropout, False, seed)(dnn_input)\n",
    "    deep_logit = tf.keras.layers.Dense(\n",
    "        1, use_bias=False, activation=None)(deep_out)\n",
    "    final_logit = tf.keras.layers.add([deep_logit, linear_logit])\n",
    "    output = PredictionLayer(task)(final_logit)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inputs_list,\n",
    "                                  outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixlen_feature_columns = [SparseFeat(feat, sam_data[feat].nunique())\n",
    "                           for feat in sparse_features] + [DenseFeat(feat, 1,)\n",
    "                          for feat in dense_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseFeat(name='C1', dimension=27, dtype='int32', embedding_name='C1', embedding=True),\n",
       " SparseFeat(name='C2', dimension=92, dtype='int32', embedding_name='C2', embedding=True),\n",
       " SparseFeat(name='C3', dimension=172, dtype='int32', embedding_name='C3', embedding=True),\n",
       " SparseFeat(name='C4', dimension=157, dtype='int32', embedding_name='C4', embedding=True),\n",
       " SparseFeat(name='C5', dimension=12, dtype='int32', embedding_name='C5', embedding=True),\n",
       " SparseFeat(name='C6', dimension=7, dtype='int32', embedding_name='C6', embedding=True),\n",
       " SparseFeat(name='C7', dimension=183, dtype='int32', embedding_name='C7', embedding=True),\n",
       " SparseFeat(name='C8', dimension=19, dtype='int32', embedding_name='C8', embedding=True),\n",
       " SparseFeat(name='C9', dimension=2, dtype='int32', embedding_name='C9', embedding=True),\n",
       " SparseFeat(name='C10', dimension=142, dtype='int32', embedding_name='C10', embedding=True),\n",
       " SparseFeat(name='C11', dimension=173, dtype='int32', embedding_name='C11', embedding=True),\n",
       " SparseFeat(name='C12', dimension=170, dtype='int32', embedding_name='C12', embedding=True),\n",
       " SparseFeat(name='C13', dimension=166, dtype='int32', embedding_name='C13', embedding=True),\n",
       " SparseFeat(name='C14', dimension=14, dtype='int32', embedding_name='C14', embedding=True),\n",
       " SparseFeat(name='C15', dimension=170, dtype='int32', embedding_name='C15', embedding=True),\n",
       " SparseFeat(name='C16', dimension=168, dtype='int32', embedding_name='C16', embedding=True),\n",
       " SparseFeat(name='C17', dimension=9, dtype='int32', embedding_name='C17', embedding=True),\n",
       " SparseFeat(name='C18', dimension=127, dtype='int32', embedding_name='C18', embedding=True),\n",
       " SparseFeat(name='C19', dimension=44, dtype='int32', embedding_name='C19', embedding=True),\n",
       " SparseFeat(name='C20', dimension=4, dtype='int32', embedding_name='C20', embedding=True),\n",
       " SparseFeat(name='C21', dimension=169, dtype='int32', embedding_name='C21', embedding=True),\n",
       " SparseFeat(name='C22', dimension=6, dtype='int32', embedding_name='C22', embedding=True),\n",
       " SparseFeat(name='C23', dimension=10, dtype='int32', embedding_name='C23', embedding=True),\n",
       " SparseFeat(name='C24', dimension=125, dtype='int32', embedding_name='C24', embedding=True),\n",
       " SparseFeat(name='C25', dimension=20, dtype='int32', embedding_name='C25', embedding=True),\n",
       " SparseFeat(name='C26', dimension=90, dtype='int32', embedding_name='C26', embedding=True),\n",
       " DenseFeat(name='I1', dimension=1, dtype='float32'),\n",
       " DenseFeat(name='I2', dimension=1, dtype='float32'),\n",
       " DenseFeat(name='I3', dimension=1, dtype='float32'),\n",
       " DenseFeat(name='I4', dimension=1, dtype='float32'),\n",
       " DenseFeat(name='I5', dimension=1, dtype='float32'),\n",
       " DenseFeat(name='I6', dimension=1, dtype='float32'),\n",
       " DenseFeat(name='I7', dimension=1, dtype='float32'),\n",
       " DenseFeat(name='I8', dimension=1, dtype='float32'),\n",
       " DenseFeat(name='I9', dimension=1, dtype='float32'),\n",
       " DenseFeat(name='I10', dimension=1, dtype='float32'),\n",
       " DenseFeat(name='I11', dimension=1, dtype='float32'),\n",
       " DenseFeat(name='I12', dimension=1, dtype='float32'),\n",
       " DenseFeat(name='I13', dimension=1, dtype='float32')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixlen_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_feature_columns = fixlen_feature_columns\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "fixlen_feature_names = get_fixlen_feature_names(linear_feature_columns + dnn_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C1',\n",
       " 'C2',\n",
       " 'C3',\n",
       " 'C4',\n",
       " 'C5',\n",
       " 'C6',\n",
       " 'C7',\n",
       " 'C8',\n",
       " 'C9',\n",
       " 'C10',\n",
       " 'C11',\n",
       " 'C12',\n",
       " 'C13',\n",
       " 'C14',\n",
       " 'C15',\n",
       " 'C16',\n",
       " 'C17',\n",
       " 'C18',\n",
       " 'C19',\n",
       " 'C20',\n",
       " 'C21',\n",
       " 'C22',\n",
       " 'C23',\n",
       " 'C24',\n",
       " 'C25',\n",
       " 'C26',\n",
       " 'I1',\n",
       " 'I2',\n",
       " 'I3',\n",
       " 'I4',\n",
       " 'I5',\n",
       " 'I6',\n",
       " 'I7',\n",
       " 'I8',\n",
       " 'I9',\n",
       " 'I10',\n",
       " 'I11',\n",
       " 'I12',\n",
       " 'I13']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixlen_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(sam_data, test_size=0.2)\n",
    "train_model_input = [train[name] for name in fixlen_feature_names]\n",
    "test_model_input = [test[name] for name in fixlen_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'pandas.core.series.Series'>\"}), <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128 samples, validate on 32 samples\n",
      "Epoch 1/20\n",
      "128/128 - 3s - loss: 0.7224 - binary_crossentropy: 0.7224 - val_loss: 0.6875 - val_binary_crossentropy: 0.6874\n",
      "Epoch 2/20\n",
      "128/128 - 0s - loss: 0.7006 - binary_crossentropy: 0.7005 - val_loss: 0.6732 - val_binary_crossentropy: 0.6732\n",
      "Epoch 3/20\n",
      "128/128 - 0s - loss: 0.6804 - binary_crossentropy: 0.6804 - val_loss: 0.6600 - val_binary_crossentropy: 0.6600\n",
      "Epoch 4/20\n",
      "128/128 - 0s - loss: 0.6614 - binary_crossentropy: 0.6614 - val_loss: 0.6475 - val_binary_crossentropy: 0.6475\n",
      "Epoch 5/20\n",
      "128/128 - 0s - loss: 0.6432 - binary_crossentropy: 0.6432 - val_loss: 0.6354 - val_binary_crossentropy: 0.6354\n",
      "Epoch 6/20\n",
      "128/128 - 0s - loss: 0.6256 - binary_crossentropy: 0.6256 - val_loss: 0.6238 - val_binary_crossentropy: 0.6237\n",
      "Epoch 7/20\n",
      "128/128 - 0s - loss: 0.6083 - binary_crossentropy: 0.6083 - val_loss: 0.6125 - val_binary_crossentropy: 0.6125\n",
      "Epoch 8/20\n",
      "128/128 - 0s - loss: 0.5911 - binary_crossentropy: 0.5910 - val_loss: 0.6015 - val_binary_crossentropy: 0.6014\n",
      "Epoch 9/20\n",
      "128/128 - 0s - loss: 0.5738 - binary_crossentropy: 0.5737 - val_loss: 0.5906 - val_binary_crossentropy: 0.5906\n",
      "Epoch 10/20\n",
      "128/128 - 0s - loss: 0.5563 - binary_crossentropy: 0.5562 - val_loss: 0.5800 - val_binary_crossentropy: 0.5800\n",
      "Epoch 11/20\n",
      "128/128 - 0s - loss: 0.5385 - binary_crossentropy: 0.5385 - val_loss: 0.5697 - val_binary_crossentropy: 0.5697\n",
      "Epoch 12/20\n",
      "128/128 - 0s - loss: 0.5204 - binary_crossentropy: 0.5203 - val_loss: 0.5598 - val_binary_crossentropy: 0.5597\n",
      "Epoch 13/20\n",
      "128/128 - 0s - loss: 0.5019 - binary_crossentropy: 0.5018 - val_loss: 0.5502 - val_binary_crossentropy: 0.5501\n",
      "Epoch 14/20\n",
      "128/128 - 0s - loss: 0.4830 - binary_crossentropy: 0.4830 - val_loss: 0.5412 - val_binary_crossentropy: 0.5411\n",
      "Epoch 15/20\n",
      "128/128 - 0s - loss: 0.4638 - binary_crossentropy: 0.4638 - val_loss: 0.5330 - val_binary_crossentropy: 0.5330\n",
      "Epoch 16/20\n",
      "128/128 - 0s - loss: 0.4443 - binary_crossentropy: 0.4443 - val_loss: 0.5258 - val_binary_crossentropy: 0.5257\n",
      "Epoch 17/20\n",
      "128/128 - 0s - loss: 0.4247 - binary_crossentropy: 0.4246 - val_loss: 0.5197 - val_binary_crossentropy: 0.5196\n",
      "Epoch 18/20\n",
      "128/128 - 0s - loss: 0.4048 - binary_crossentropy: 0.4048 - val_loss: 0.5150 - val_binary_crossentropy: 0.5150\n",
      "Epoch 19/20\n",
      "128/128 - 0s - loss: 0.3848 - binary_crossentropy: 0.3848 - val_loss: 0.5120 - val_binary_crossentropy: 0.5119\n",
      "Epoch 20/20\n",
      "128/128 - 0s - loss: 0.3648 - binary_crossentropy: 0.3647 - val_loss: 0.5106 - val_binary_crossentropy: 0.5106\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'pandas.core.series.Series'>\"}), <class 'NoneType'>\n",
      "test LogLoss 0.6163\n",
      "test AUC 0.5923\n"
     ]
    }
   ],
   "source": [
    "model = FNN(linear_feature_columns, dnn_feature_columns, task='binary')\n",
    "model.compile(\"adam\", \"binary_crossentropy\",\n",
    "                  metrics=['binary_crossentropy'], )\n",
    "\n",
    "history = model.fit(train_model_input, train[target].values,\n",
    "                        batch_size=256, epochs=20, verbose=2, validation_split=0.2, )\n",
    "pred_ans = model.predict(test_model_input, batch_size=256)\n",
    "print(\"test LogLoss\", round(log_loss(test[target].values, pred_ans), 4))\n",
    "print(\"test AUC\", round(roc_auc_score(test[target].values, pred_ans), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
